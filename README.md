# ClusterHikers
CISPA Hackathon 2025 - Team 42

## Overview

This repository contains our work for the CISPA Hackathon 2025, focusing on AI/ML security topics including Dataset Inference, LLM Text Attribution, and Watermarking techniques for generative models.

---

## Table of Contents

1. [Dataset Inference](#dataset-inference)
2. [LLM Text Attribution](#llm-text-attribution)
3. [Watermarking Techniques](#watermarking-techniques)
4. [Image Autoregressive Models](#image-autoregressive-models)
5. [References](#references)

---

## Dataset Inference

Dataset inference aims to determine whether specific data was used to train a machine learning model, addressing ownership and privacy concerns in ML systems.

### 1. Dataset Inference: Ownership Resolution (Maini et al., ICLR 2021)

**Core Concept**: Determine if a dataset was used to train a model by analyzing the model's predictions on samples from the suspected dataset versus reference datasets.

**Key Method - Likelihood Ratio Test**:

The approach uses a likelihood ratio test to compare two hypotheses:
- **H₀** (Null): Model was trained on a reference dataset D_ref
- **H₁** (Alternative): Model was trained on the suspected dataset D_sus

The test statistic is computed as:

```
Λ = L(D_sus | f) / L(D_ref | f)
```

Where `L(D | f)` represents the likelihood of the data D given model f.

**Prediction Entropy Method**:

For classification tasks, the entropy of predictions on a dataset D is computed:

```
H(D) = -(1/|D|) Σ_{x∈D} Σ_c p_c(x) log(p_c(x))
```

Where:
- `p_c(x)` is the predicted probability for class c on input x
- Lower entropy on D_sus compared to D_ref suggests D_sus was used in training

**Membership Inference Score**:

For individual samples, a membership score is calculated:

```
s(x) = log p(x | f_sus) - log p(x | f_ref)
```

Where positive scores indicate higher likelihood that x was in the training set.

### 2. LLM Dataset Inference (Maini et al., NeurIPS 2024)

**Core Concept**: Extend dataset inference to Large Language Models by analyzing perplexity and loss distributions across different text corpora.

**Perplexity-Based Detection**:

The perplexity of a language model on a sequence is:

```
PPL(x₁, ..., xₙ) = exp(-(1/n) Σᵢ log P(xᵢ | x₁, ..., xᵢ₋₁))
```

Where:
- Lower perplexity on a corpus suggests it was used in training
- Comparison threshold: `PPL(D_sus) < τ · PPL(D_ref)`

**Min-K% Prob Method**:

Select the k% tokens with lowest prediction probability in a sequence:

```
score(x) = (1/k) Σᵢ∈MinK log P(xᵢ | x₁, ..., xᵢ₋₁)
```

This metric is more robust to distribution shifts than average perplexity.

**Likelihood Ratio for Text**:

```
LR = Σᵢ [log P(xᵢ | x₁, ..., xᵢ₋₁; θ_sus) - log P(xᵢ | x₁, ..., xᵢ₋₁; θ_ref)]
```

### 3. Dataset Inference for Self-Supervised Models (Dziedzic et al., NeurIPS 2022)

**Core Concept**: Detect training data for self-supervised models (e.g., contrastive learning, MAE) using representation-based analysis.

**Contrastive Learning (SimCLR, MoCo)**:

For a query sample x and its augmentation x⁺:

```
sim(x, x⁺) = exp(z(x) · z(x⁺) / τ) / Σₖ exp(z(x) · z(xₖ) / τ)
```

Where:
- `z(·)` is the learned representation
- `τ` is temperature parameter
- Higher similarity for training data

**Representation Distance**:

```
d(x) = ||φ(x) - φ̄(D_ref)||₂
```

Where:
- `φ(x)` is the embedding of x
- `φ̄(D_ref)` is the centroid of reference embeddings
- Smaller distances indicate training set membership

**Reconstruction Error (for MAE-type models)**:

```
L_recon(x) = ||x - decoder(encoder(mask(x)))||₂²
```

Lower reconstruction error suggests x was in the training set.

---

## LLM Text Attribution

Text attribution determines whether text was generated by an LLM or written by humans, and which specific model generated it.

### 1. Survey of LLM Attribution (Li et al.)

**Core Approaches**:

**A. Watermarking-Based Attribution**:

Embed imperceptible signals during text generation. A common approach uses logit modification:

```
logit'(w) = logit(w) + δ · h(w, context)
```

Where:
- `h(w, context)` is a hash function determining watermark strength
- `δ` controls watermark intensity

**B. Statistical Detection**:

Based on distribution differences. The log-likelihood ratio:

```
S(x) = Σᵢ [log P_LLM(xᵢ | x₁, ..., xᵢ₋₁) - log P_human(xᵢ | x₁, ..., xᵢ₋₁)]
```

Higher scores suggest LLM generation.

**C. Perplexity-Based Detection**:

```
PPL_threshold = E[PPL(X_human)] + k · σ[PPL(X_human)]
```

If `PPL(x) < PPL_threshold`, classify as LLM-generated (LLMs produce lower perplexity text).

**D. Burstiness Metric**:

Measures token probability variance:

```
B(x) = Var(log P(xᵢ | context)) / E[log P(xᵢ | context)]²
```

Human text typically shows higher burstiness.

### 2. XAI-Based Attribution (Najjar et al.)

**Core Concept**: Use explainable AI techniques (SHAP, LIME) to identify features distinguishing human vs. LLM text.

**Feature Attribution Score**:

For a feature f (e.g., word, n-gram):

```
φᵢ(x) = E[f(x | Sᵢ ∪ {i})] - E[f(x | Sᵢ)]
```

Where:
- `Sᵢ` is a subset of features
- `φᵢ` measures marginal contribution of feature i

**Multi-Model Distinction**:

For distinguishing between K different LLMs:

```
score_k(x) = Σᵢ wᵢ · fᵢ(x)
```

Where:
- `wᵢ` are learned weights from training classifier
- `fᵢ(x)` are extracted features (syntactic, semantic, statistical)

**Classification Decision**:

```
ŷ = argmax_k P(model = k | x) = argmax_k softmax(score_k(x))
```

---

## Watermarking Techniques

Watermarking embeds invisible or imperceptible signatures in generated content for provenance tracking and ownership verification.

### 1. BitMark: Watermarking Bitwise Autoregressive Models (Kerner et al., NeurIPS 2025)

**Core Concept**: Embed watermarks in bitwise autoregressive image generation by biasing bit predictions using a secret key.

**Bit Generation Process**:

Standard autoregressive bit generation:

```
P(b_t | b_{<t}) = σ(f_θ(b_{<t}))
```

Where `σ` is sigmoid, `f_θ` is the model, `b_t` is the t-th bit.

**Watermark Embedding**:

Modify logits using a keyed pseudo-random function:

```
logit'(b_t) = logit(b_t) + λ · PRF(key, position_t, context)
```

Where:
- `λ` controls watermark strength
- `PRF` is a pseudo-random function (e.g., keyed hash)
- `position_t` identifies the bit position

**Detection Statistic**:

For suspected watermarked image with bits B = {b₁, ..., b_n}:

```
z = (1/n) Σₜ (2·b_t - 1) · PRF(key, position_t, context)
```

Under null hypothesis (no watermark): `z ~ N(0, 1/√n)`
Under alternative (watermarked): `z ~ N(λ/√n, 1/√n)`

**Detection Decision**:

```
H₁ if z > threshold, else H₀
```

With threshold set for desired false positive rate using z-test.

### 2. Tree-Rings: Invisible Fingerprints for Diffusion Models (Wen et al., NeurIPS 2023)

**Core Concept**: Embed watermarks in the initial noise of diffusion models, which propagates through the generation process.

**Diffusion Forward Process**:

```
q(x_t | x_0) = N(x_t; √(ᾱ_t)x_0, (1-ᾱ_t)I)
```

Where `ᾱ_t` controls noise schedule.

**Watermarked Initial Noise**:

```
z_T = z_T^random + w · pattern(key)
```

Where:
- `z_T^random` is standard Gaussian noise
- `pattern(key)` is a Fourier-space pattern generated from secret key
- `w` is watermark strength

**Fourier Space Pattern**:

```
F(pattern)(r, θ) = A · δ(r - r_key(θ))
```

Creates a "tree ring" pattern in Fourier domain at radius determined by key.

**Reverse Diffusion (Generation)**:

```
x_{t-1} = (1/√α_t)(x_t - ((1-α_t)/√(1-ᾱ_t))ε_θ(x_t, t)) + σ_t z
```

The watermark pattern propagates through to final image x₀.

**Detection**:

Compute Fourier transform of image:

```
score = |F(x_0) · F(pattern(key))| / ||F(x_0)||
```

High score indicates watermark presence.

### 3. Stable Signature: Watermarks in Latent Diffusion (Fernandez et al., ICCV 2023)

**Core Concept**: Train an additional decoder to embed watermarks in the latent space of models like Stable Diffusion.

**Latent Diffusion Generation**:

```
z_0 = DDIM(z_T, prompt) where z_0 ∈ latent space
x_0 = Decoder(z_0) where x_0 ∈ image space
```

**Watermark Embedding**:

Train watermark encoder `E_w` and decoder `D_w`:

```
z_w = z_0 + E_w(msg, z_0)
x_w = Decoder(z_w)
```

Where `msg` is the watermark message.

**Training Objective**:

```
L = L_image + α · L_msg + β · L_imperceptible
```

Where:
- `L_image = ||x_w - x_0||` (preserve image quality)
- `L_msg = CrossEntropy(D_w(x_w), msg)` (watermark detectability)
- `L_imperceptible = LPIPS(x_w, x_0)` (perceptual similarity)

**Detection**:

```
msg_extracted = D_w(x_w)
match = (msg_extracted == msg)
```

**Robustness**:

Watermark survives transformations T (crop, compression, noise):

```
msg_extracted = D_w(T(x_w))
accuracy = P(msg_extracted == msg | T)
```

---

## Image Autoregressive Models

These models generate images by predicting next tokens/scales autoregressively, similar to language models.

### 1. Visual Autoregressive (VAR) Modeling (Tian et al., NeurIPS 2024)

**Core Concept**: Generate images by predicting progressively finer scales, rather than raster order.

**Multi-Scale Representation**:

Image is tokenized at multiple resolutions:

```
{r₁, r₂, ..., r_L} where r_l ∈ {1, ..., V}^(h_l × w_l)
```

**Next-Scale Prediction**:

```
P(r_l | r_{<l}) = Π_{i,j} P(r_l^{i,j} | r_{<l}, r_l^{<i,j})
```

Predicts entire scale l conditioned on all previous scales.

**Advantages**:
- More efficient than raster-order autoregression
- Better captures hierarchical structure
- Scalable to high resolutions

### 2. Randomized Autoregressive (RAR) Visual Generation (Yu et al., 2024)

**Core Concept**: Use random ordering instead of fixed raster order for more flexible generation.

**Random Permutation**:

```
π ~ Uniform(Permutations(n))
x_π = permute(x, π)
```

**Training Objective**:

```
L = E_π [Σᵢ -log P(x_π(i) | x_π(<i))]
```

Average over random orderings.

**Generation**:

Can generate in any order, enabling:
- Parallel decoding of independent tokens
- Conditional inpainting
- Flexible editing

### 3. Infinity: Bitwise AutoRegressive Modeling (Han et al., CVPR 2025)

**Core Concept**: Generate images bit-by-bit for precise control and high resolution.

**Bitwise Representation**:

Each pixel value decomposed to bits:

```
pixel_value = Σᵢ b_i · 2^i where b_i ∈ {0, 1}
```

**Bitwise Autoregression**:

```
P(image) = Π_pixels Π_bits P(b_{pixel,bit} | previous_bits)
```

**Advantages**:
- Ultra-high resolution capability
- Precise pixel control
- Compatible with BitMark watermarking

---

## References

### Dataset Inference
1. [Dataset Inference: Ownership Resolution in Machine Learning](https://openreview.net/pdf?id=hvdKKV2yt7T) - Maini et al., ICLR 2021
2. [LLM Dataset Inference: Did you train on my dataset?](https://adam-dziedzic.com/static/assets/papers/LLMDatasetInference.pdf) - Maini et al., NeurIPS 2024
3. [Dataset Inference for Self-Supervised Models](https://adam-dziedzic.com/static/assets/papers/dataset-inference-ssl.pdf) - Dziedzic et al., NeurIPS 2022

### Text Attribution
1. [A Survey of Large Language Models Attribution](https://arxiv.org/abs/2311.03731) - Li et al., 2024
2. [Leveraging Explainable AI for LLM Text Attribution](https://arxiv.org/abs/2501.03212) - Najjar et al., 2025

### Watermarking
1. [BitMark: Watermarking Bitwise Autoregressive Image Generative Models](https://openreview.net/forum?id=VSir0FzFnP) - Kerner et al., NeurIPS 2025
2. [Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images](https://openreview.net/forum?id=Z57JrmubNl) - Wen et al., NeurIPS 2023
3. [The Stable Signature: Rooting Watermarks in Latent Diffusion Models](https://arxiv.org/abs/2303.15435) - Fernandez et al., ICCV 2023

### Image Autoregressive Models
1. [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://openreview.net/forum?id=gojL67CfS8) - Tian et al., NeurIPS 2024
2. [Randomized Autoregressive Visual Generation](https://yucornetto.github.io/projects/rar.html) - Yu et al., 2024
3. [Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis](https://arxiv.org/abs/2412.04431) - Han et al., CVPR 2025

---

## Setup and Usage

For setup instructions including SLURM configuration, UV package management, and cluster access, see [notes.txt](notes.txt).

---

## Team

**Team 42 - ClusterHikers**  
CISPA Hackathon 2025
